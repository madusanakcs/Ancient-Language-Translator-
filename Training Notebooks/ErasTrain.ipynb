{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11533358,"sourceType":"datasetVersion","datasetId":7233736}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom PIL import Image, ImageOps\n\ndef preprocess_bw(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    _, thresh = cv2.threshold(blurred, 127, 255, cv2.THRESH_BINARY)\n    return thresh\n\ndef pad_image(image, border):\n    # Ensure image is uint8\n    if image.dtype != np.uint8:\n        image = np.clip(image, 0, 255).astype(np.uint8)\n\n    # Skip if image is too small\n    if image.shape[0] < 2 or image.shape[1] < 2:\n        print(f\"âš ï¸ Skipping padding: image too small ({image.shape})\")\n        return image\n\n    try:\n        pil_img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        padded = ImageOps.expand(pil_img, border=border, fill='white')\n        padded_np = np.array(padded)\n        # Ensure final padded image is uint8\n        padded_np = np.clip(padded_np, 0, 255).astype(np.uint8)\n        return cv2.cvtColor(padded_np, cv2.COLOR_RGB2BGR)\n    except Exception as e:\n        print(f\"âŒ pad_image error: {e}\")\n        return image\n\n\ndef crop_image(image, crop_border):\n    h, w = image.shape[:2]\n    return image[crop_border:h - crop_border, crop_border:w - crop_border]\n\ndef rotate_image(image, angle, size):\n    M = cv2.getRotationMatrix2D((size[0] / 2, size[1] / 2), angle, 1)\n    return cv2.warpAffine(image, M, size, borderValue=(255, 255, 255))\n\n\n\ndef augment_image(image):\n    rows, cols, _ = image.shape\n    target_size = (cols, rows)\n\n    # Base augmentations\n    rotated_pos15 = rotate_image(image, 15, target_size)\n    rotated_neg15 = rotate_image(image, -15, target_size)\n\n    scaled = cv2.resize(image, None, fx=1.2, fy=1.2)\n    scaled = cv2.resize(scaled, target_size, interpolation=cv2.INTER_AREA)\n\n    M_translate = np.float32([[1, 0, 20], [0, 1, 30]])\n    translated = cv2.warpAffine(image, M_translate, target_size, borderValue=(255, 255, 255))\n\n    noise_std = 15\n    noise = np.random.normal(0, noise_std, image.shape).astype(np.int16)\n    noisy_image = np.clip(image.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n\n\n\n    padded_30 = cv2.resize(pad_image(image, 30), target_size)\n    padded_60 = cv2.resize(pad_image(image, 60), target_size)\n    padded_30_rot15 = rotate_image(padded_30, 15, target_size)\n    padded_60_rot_neg15 = rotate_image(padded_60, -15, target_size)\n\n    cropped_20 = cv2.resize(crop_image(image, 20), target_size)\n    cropped_40 = cv2.resize(crop_image(image, 40), target_size)\n    cropped_50 = cv2.resize(crop_image(image, 50), target_size)\n    cropped_20_rot15 = rotate_image(cropped_20, 15, target_size)\n    cropped_20_rot_neg15 = rotate_image(cropped_20, -15, target_size)\n\n    images = [\n        image,\n        rotated_pos15,\n        rotated_neg15,\n        scaled,\n        translated,\n        noisy_image,\n        \n        padded_30,\n        padded_60,\n        padded_30_rot15,\n        padded_60_rot_neg15,\n        cropped_20,\n        cropped_40,\n        cropped_50,\n        cropped_20_rot15,\n        cropped_20_rot_neg15\n    ]\n\n    bw_images = [preprocess_bw(img) for img in images]\n\n    bw_rgb_images = [cv2.cvtColor(bw_img, cv2.COLOR_GRAY2RGB) for bw_img in bw_images]\n\n    # Convert all images to uint8 (very important)\n    bw_rgb_images = [img.astype(np.uint8) for img in bw_rgb_images]\n\n    return bw_rgb_images\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-03T10:17:46.761184Z","iopub.execute_input":"2025-06-03T10:17:46.761552Z","iopub.status.idle":"2025-06-03T10:17:47.269598Z","shell.execute_reply.started":"2025-06-03T10:17:46.761522Z","shell.execute_reply":"2025-06-03T10:17:47.268139Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport os\nimport joblib\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNetV2\n\n# Assuming you have these functions defined elsewhere\n# from your_utils import load_dataset_kaggle, load_images_with_preprocessing, augment_image\n\ndef process_and_train(dataset_num):\n    data_dir = f'/kaggle/input/lettereras/{dataset_num}'\n    \n    if not os.path.exists(data_dir):\n        return  # skip if folder doesn't exist\n    \n    try:\n        # Load and preprocess data\n        X, y = load_images_with_preprocessing(data_dir)\n        \n        # Data augmentation\n        X_aug = []\n        y_aug = []\n        for img, label in zip(X, y):\n            augmented_images = augment_image(img)\n            for aug_img in augmented_images:\n                X_aug.append(aug_img)\n                y_aug.append(label)\n        \n        X_aug = np.array(X_aug)\n        y_aug = np.array(y_aug)\n        \n        # Encode labels\n        label_encoder = LabelEncoder()\n        y_encoded = label_encoder.fit_transform(y_aug)\n        y_categorical = to_categorical(y_encoded)\n        \n        # Save label encoder\n        joblib.dump(label_encoder, f'/kaggle/working/label_encoder_{dataset_num}.pkl')\n\n        \n        #X_aug = (X_aug / 255).astype(np.float32)\n        print(X_aug.shape)\n        \n        # Train-test split\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_aug, y_categorical, test_size=0.1, \n            stratify=y_encoded, random_state=36\n        )\n        \n        # Build model\n        base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n        for layer in base_model.layers:\n            layer.trainable = False\n        \n        x = base_model.output\n        x = GlobalAveragePooling2D()(x)\n        x = Dropout(0.5)(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        predictions = Dense(y_categorical.shape[1], activation='softmax')(x)\n        \n        model = Model(inputs=base_model.input, outputs=predictions)\n        model.compile(\n            optimizer=Adam(learning_rate=1e-3),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        # Train model (no logs)\n        model.fit(\n            X_train, y_train,\n            validation_data=(X_val, y_val),\n            epochs=50,\n            batch_size=8,\n            verbose=1  # hide training logs\n        )\n        \n        # Save model\n        model.save(f'/kaggle/working/model_{dataset_num}.h5')\n        print(f\"âœ… Done training & saving dataset {dataset_num}\")\n\n    except Exception as e:\n        print(f\"âŒ Error with dataset {dataset_num}: {str(e)}\")\n\n# Process all datasets from 0 to 36\nfor i in range(37):  # 0 to 36 inclusive\n    process_and_train(i)\n\nprint(\"ðŸŽ‰ All datasets processed successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}